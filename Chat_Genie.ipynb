{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzGqhijgngtq",
        "outputId": "c1f9734e-14e1-411f-e65e-1ee83f5ece61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.1.1)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.6.4)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.28.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.25.2)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.10.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.23.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.23.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.23.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.2)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.3.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.62.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.23.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.23.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.23.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.23.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=3da097e5bd8f6c17a468ae57f713bb3f88922114804ce135f561548bb87eae93\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, python-dotenv, pulsar-client, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.0.2\n",
            "    Uninstalling importlib_metadata-7.0.2:\n",
            "      Successfully uninstalled importlib_metadata-7.0.2\n",
            "Successfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.110.0 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-metadata-6.11.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 onnxruntime-1.17.1 opentelemetry-api-1.23.0 opentelemetry-exporter-otlp-proto-common-1.23.0 opentelemetry-exporter-otlp-proto-grpc-1.23.0 opentelemetry-instrumentation-0.44b0 opentelemetry-instrumentation-asgi-0.44b0 opentelemetry-instrumentation-fastapi-0.44b0 opentelemetry-proto-1.23.0 opentelemetry-sdk-1.23.0 opentelemetry-semantic-conventions-0.44b0 opentelemetry-util-http-0.44b0 orjson-3.9.15 overrides-7.7.0 posthog-3.5.0 pulsar-client-3.4.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.36.3 uvicorn-0.28.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Collecting fuzzywuzzy[speedup]\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting python-levenshtein>=0.12 (from fuzzywuzzy[speedup])\n",
            "  Downloading python_Levenshtein-0.25.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.25.0 (from python-levenshtein>=0.12->fuzzywuzzy[speedup])\n",
            "  Downloading Levenshtein-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.25.0->python-levenshtein>=0.12->fuzzywuzzy[speedup])\n",
            "  Downloading rapidfuzz-3.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-levenshtein\n",
            "Successfully installed Levenshtein-0.25.0 fuzzywuzzy-0.18.0 python-levenshtein-0.25.0 rapidfuzz-3.6.2\n",
            "Collecting openai\n",
            "  Downloading openai-1.14.1-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: httpcore, httpx, openai\n",
            "Successfully installed httpcore-1.0.4 httpx-0.27.0 openai-1.14.1\n",
            "Collecting SQLAlchemy==1.4.46\n",
            "  Downloading SQLAlchemy-1.4.46-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy==1.4.46) (3.0.3)\n",
            "Installing collected packages: SQLAlchemy\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.28\n",
            "    Uninstalling SQLAlchemy-2.0.28:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.28\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.46 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SQLAlchemy-1.4.46\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (20.11.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb\n",
        "!pip install fuzzywuzzy[speedup]\n",
        "!pip install openai\n",
        "!pip install SQLAlchemy==1.4.46\n",
        "!pip install sqlglot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fyDAP0uJnpZZ"
      },
      "outputs": [],
      "source": [
        "# Import Chroma and instantiate a client. The default Chroma client is ephemeral, meaning it will not save to disk.\n",
        "import chromadb\n",
        "import spacy\n",
        "from fuzzywuzzy import process ,fuzz\n",
        "from openai import OpenAI\n",
        "import re\n",
        "import sqlalchemy\n",
        "from sqlalchemy import create_engine, text, sql\n",
        "import pandas as pd\n",
        "from chromadb.config import Settings\n",
        "import openai\n",
        "import sqlglot\n",
        "from sqlglot.optimizer import optimize\n",
        "from chromadb.utils import embedding_functions\n",
        "from typing import List\n",
        "import warnings\n",
        "import os\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from matplotlib import pyplot as plt\n",
        "import logging\n",
        "import plotly.express as px\n",
        "import plotly.subplots as sp\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)  # Unlimited columns\n",
        "pd.set_option('display.width', 500)     # Width of the display in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cRjTG5Pvo8hH"
      },
      "outputs": [],
      "source": [
        "dataset = [{\n",
        "    \"question\":\"total number of orders for the last 30 days\",\n",
        "    \"sql\":'''\n",
        "            select date(created_at) as date,\n",
        "            count(distinct id) as orders\n",
        "\n",
        "            from orders\n",
        "            where date(created_at) between current_date-30 and current_date\n",
        "            group by 1\n",
        "   '''},\n",
        "            {\n",
        "            \"question\": \"what is average time interval between first and second order for last 180 days?\" ,\n",
        "            \"sql\": '''\n",
        "            with base as (\n",
        "                select email , created_at , dense_rank() over (partition by email order by created_at asc) as order_number\n",
        "                from orders\n",
        "                where created_at >= current_date-180\n",
        "                order by 1,3\n",
        "                ) ,\n",
        "            user_base as (\n",
        "                select email , order_number , created_at as current_order , lead(created_at) over(partition by email order by created_at) as next_order\n",
        "                from base\n",
        "            ),\n",
        "            final_base as (\n",
        "                select email , next_order-current_order as time_interval\n",
        "                from user_base\n",
        "                where order_number = 1\n",
        "            )\n",
        "            select avg(time_interval) from final_base\n",
        "            '''\n",
        "        } ,\n",
        "                {\n",
        "            \"question\": \"what is median time interval between first and second order for last 180 days?\" ,\n",
        "            \"sql\": '''\n",
        "            with base as (\n",
        "                select email , created_at , dense_rank() over (partition by email order by created_at asc) as order_number\n",
        "                from orders\n",
        "                where created_at >= current_date-180\n",
        "                order by 1,3\n",
        "                ) ,\n",
        "            user_base as (\n",
        "                select email , order_number , created_at as current_order , lead(created_at) over(partition by email order by created_at) as next_order\n",
        "                from base\n",
        "            ),\n",
        "            final_base as (\n",
        "                select email , next_order-current_order as time_interval\n",
        "                from user_base\n",
        "                where order_number = 1\n",
        "            )\n",
        "            select percentile_cont(0.50) within group (order by time_interval) from final_base\n",
        "            '''\n",
        "        },\n",
        "\n",
        "        {\n",
        "        \"question\": \"what is average time interval between two orders for last 90 days?\" ,\n",
        "        \"sql\": '''\n",
        "        with base as (\n",
        "            select email , created_at\n",
        "            from orders\n",
        "            where created_at >= current_date-90\n",
        "            group by 1,2\n",
        "            order by 1,2\n",
        "            ) ,\n",
        "        lag_base as (\n",
        "            select email ,created_at as new_order , LAG(created_at) OVER (PARTITION BY email ORDER BY created_at) as prior_order\n",
        "        from base\n",
        "        ),\n",
        "        final_base as (\n",
        "            select email , new_order-prior_order as time_between_two_orders from lag_base\n",
        "        )\n",
        "        select avg(time_between_two_orders) from final_base\n",
        "        '''\n",
        "    } ,\n",
        "\n",
        "     {\n",
        "        \"question\": \"what is median time interval between two orders for last 90 days?\" ,\n",
        "        \"sql\": '''\n",
        "        with base as (\n",
        "            select email , created_at\n",
        "            from orders\n",
        "            where created_at >= current_date-90\n",
        "            group by 1,2\n",
        "            order by 1,2\n",
        "            ) ,\n",
        "        lag_base as (\n",
        "            select email ,created_at as new_order , LAG(created_at) OVER (PARTITION BY email ORDER BY created_at) as prior_order\n",
        "        from base\n",
        "        ),\n",
        "        final_base as (\n",
        "            select email , new_order-prior_order as time_between_two_orders from lag_base\n",
        "        )\n",
        "        select percentile_cont(0.50) within group (order by time_between_two_orders) from final_base\n",
        "        '''\n",
        "    } ,\n",
        "\n",
        "\n",
        "    {\n",
        "        \"question\" : \"what is my month on month growth rate of orders?\",\n",
        "        \"sql\" : '''\n",
        "        with base as (\n",
        "        select date_trunc('MONTH',created_at)::DATE as month , count(distinct id) as orders\n",
        "        from orders\n",
        "        group by 1\n",
        "        )\n",
        "\n",
        "        select a.month as base_month , b.month as new_month , 100.0*(b.orders-a.orders)/b.orders as growth_rate\n",
        "        from base a join base b on b.month-a.month between 26 and 32\n",
        "\n",
        "        '''\n",
        "    },\n",
        "\n",
        "    {\"question\" : \"what is my retention monthly and store wise?\" ,\n",
        "     \"sql\" : '''\n",
        "        with base as (\n",
        "            select store ,date_trunc('MONTH',created_at)::DATE as month , email from orders group by 1,2,3\n",
        "        ) ,\n",
        "\n",
        "        ret_base as (\n",
        "            select a.store , a.month as base_month, b.month as retained_month , count(distinct b.email) as retained_users\n",
        "            from base a\n",
        "            join base b on b.month >= a.month and b.email = a.email\n",
        "            group by 1,2,3\n",
        "        ),\n",
        "\n",
        "        totals_base as (\n",
        "        select a.store, a.month as base_month, count(distinct a.email) as total_users\n",
        "        from base a\n",
        "        group by 1,2)\n",
        "\n",
        "        select a.store, a.base_month, a.retained_month, a.retained_users*100.00/b.total_users as percentage_retained_users\n",
        "        from ret_base a\n",
        "        left join totals_base b on a.store=b.store and a.base_month=b.base_month\n",
        "        order by a.store, a.base_month, a.retained_month\n",
        "     '''\n",
        "     } ,\n",
        "\n",
        "   {\n",
        "       \"question\" : \"percentage of new users this month\" ,\n",
        "       \"sql\" : '''\n",
        "                with new_user_base as(\n",
        "                    select email , min(date_trunc('MONTH',created_at)::DATE) as acq_month  from orders group by 1\n",
        "                ) ,\n",
        "\n",
        "                orders_base as(\n",
        "                    select date_trunc('MONTH',created_at)::DATE as month , email from orders where date(created_at) >= date_trunc('MONTH',created_at)::DATE group by 1,2\n",
        "                )\n",
        "\n",
        "                select 100.0*count(distinct case when orders_base.month = new_user_base.acq_month then a.email end)/count(distinct orders_base.email) as new_users_percentage from orders_base a join new_user_base on orders_base.email = new_user_base.email\n",
        "\n",
        "       '''\n",
        "   } ,\n",
        "\n",
        " {\"question\" : \"how does the month on month retention percentage look like?\",\n",
        "    \"sql\" : '''\n",
        "\n",
        "    with base as (\n",
        "        select store ,(case when source_name = 'pos' then 'pos' else 'online' end) as source , date_trunc('MONTH',created_at)::DATE as month , email from orders group by 1,2,3,4\n",
        "    ) ,\n",
        "\n",
        "    ret_base as (\n",
        "        select a.store , a.source , a.month as base_month, b.month as retained_month , count(distinct b.email) as retained_users\n",
        "        from base a\n",
        "        join base b on b.month >= a.month and b.email = a.email\n",
        "        group by 1,2,3,4\n",
        "    ),\n",
        "\n",
        "    totals_base as (\n",
        "    select a.store, a.source, a.month as base_month, count(distinct a.email) as total_users\n",
        "    from base a\n",
        "    group by 1,2,3)\n",
        "\n",
        "    select a.store, a.source, a.base_month, a.retained_month, a.retained_users*100.00/b.total_users as percentage_retained_users\n",
        "    from ret_base a\n",
        "    left join totals_base b on a.store=b.store and a.source=b.source and a.base_month=b.base_month\n",
        "\n",
        "    '''} ,\n",
        "\n",
        "{\n",
        "    \"question\" : \"what is month , store and source wise retention and churn for new and existing users\" ,\n",
        "\t\"sql\" : '''\n",
        "\twith user_base as (\n",
        "        select email , min(date_trunc('MONTH',created_at)::DATE) as acq_month  from orders group by 1\n",
        "    ) ,\n",
        "\n",
        "    order_base as (\n",
        "        select store ,(case when source_name = 'pos' then 'pos' else 'online' end) as source, email , date_trunc('MONTH',created_at)::DATE as month from orders group by 1,2,3,4\n",
        "    ) ,\n",
        "\n",
        "    base as (\n",
        "        select a.store ,a.source , a.month , a.email , case when a.month = b.acq_month then 'new' else 'existing' end as type\n",
        "        from order_base a join user_base b on a.email = b.email\n",
        "    ),\n",
        "\n",
        "    base1 as (\n",
        "        select store ,source , month , count(distinct email) as total_users ,\n",
        "        count(distinct case when type = 'new' then email end) as new_users ,\n",
        "        count(distinct case when type = 'existing' then email end) as existing_users\n",
        "        from base group by 1,2,3\n",
        "    ) ,\n",
        "\n",
        "     ret_base as (\n",
        "        select a.store ,a.source , a.month as base_month ,b.month as ret_month ,\n",
        "        count(distinct case when a.type = 'new' then a.email end) as new_retained ,\n",
        "        count(distinct case when a.type = 'existing' then a.email end) as existing_retained ,\n",
        "        count(distinct a.email) as total_retained\n",
        "        from base a\n",
        "        join base b on b.month-a.month between 25 and 32 and b.email = a.email\n",
        "        group by 1,2,3,4\n",
        "     )\n",
        "\n",
        "\n",
        "\n",
        "    select a.store ,a.source , a.month as base_month , a.total_users , 100.0*a.new_users/a.total_users as new_users_share ,\n",
        "    100.0*a.existing_users/a.total_users as existing_users_share  ,\n",
        "    100.0*b.total_retained/a.total_users as retention , 100.0-(100.0*b.total_retained/a.total_users) as churn ,\n",
        "    100.0*b.new_retained/a.new_users as new_user_retention , 100.0-(100.0*b.new_retained/a.new_users) as new_user_churn ,\n",
        "    100.0*b.existing_retained/a.existing_users as existing_user_retention , 100.0-(100.0*b.existing_retained/a.existing_users) as existing_user_churn\n",
        "    from base1 a left join ret_base b on a.month = b.base_month and a.store = b.store and a.source = b.source\n",
        "  '''\n",
        "} ,\n",
        "\n",
        "\n",
        "\n",
        "{\n",
        "    \"question\":\"find the store wise number of orders for the last 30 days\",\n",
        "    \"sql\":'''\n",
        "            select date(created_at) as date,\n",
        "            store,\n",
        "            count(distinct id) as orders\n",
        "\n",
        "            from orders\n",
        "            where date(created_at) between current_date-30 and current_date\n",
        "            group by 1,2\n",
        "   '''},\n",
        "\n",
        "{\n",
        "    \"question\":\"find total sales of different product types in last 30 days\",\n",
        "    \"sql\":'''\n",
        "             select product_type,\n",
        "\n",
        "             sum(item_selling_price::float) as value_sold\n",
        "\n",
        "             from order_item\n",
        "             where product_type IN ('Personal & Home','Grains & Flour','Spices & Condiments','Instant Food & Beverages')\n",
        "             and (cast(created_at as date) between current_date-30 and current_date)\n",
        "             group by 1\n",
        "   '''},\n",
        "\n",
        "{\n",
        "    \"question\":\"find interacted users, logins, searched, collection_viewed, product_viewed, atc, ordered users information for last 30 days\",\n",
        "    \"sql\":'''with event_base as\n",
        "          (\n",
        "                select timestamp::date as date  , store , count(distinct email) as interacted_users ,\n",
        "                count(distinct case when event = 'Login' and email is not null then email end) as login ,\n",
        "                count(distinct case when event = 'Search Term' and email is not null then email end) as searched ,\n",
        "                count(distinct case when event = 'Collection Viewed' and email is not null then email end) as collection_viewed ,\n",
        "                count(distinct case when event in ('Viewed Product','Product Viewed') and email is not null then email end) as product_viewed ,\n",
        "                count(distinct case when event in ('Added To Cart') and email is not null then email end) as atc ,\n",
        "                count(distinct case when event in ('Placed Order') and email is not null then email end) as ordered\n",
        "                from events\n",
        "                where ((source_name is null) or  (source_name in ('web','app')))\n",
        "                and timestamp::date between current_date-30 and current_date\n",
        "                group by 1,2\n",
        "          ) ,\n",
        "\n",
        "          order_base as\n",
        "          (\n",
        "                select date ,store ,  count(distinct id) as orders, count(distinct email) as customers\n",
        "                from order_item\n",
        "                where ((source_name is null) or  (source_name in ('web','app')))\n",
        "                and created_at::date between current_date-30 and current_date\n",
        "                group by 1,2\n",
        "          )\n",
        "\n",
        "           select e.date , e.store , e.interacted_users , e.login ,\n",
        "           e.searched ,e.collection_viewed , e.product_viewed ,\n",
        "           e.atc , e.ordered , o.orders ,o.customers\n",
        "           from event_base e\n",
        "           left join order_base o on e.date = o.date and e.store = o.store\n",
        "           order by 2,1'''},\n",
        "\n",
        "{\n",
        "    \"question\":\"find storewise total user logins in the last 30 days\",\n",
        "    \"sql\":'''     select cast(timestamp as date) as date,\n",
        "                store,\n",
        "                count(distinct email) as login\n",
        "\n",
        "                from events\n",
        "                where (source_name is null) or  (source_name in ('web','app'))\n",
        "                and cast(timestamp as date) between current_date-30 and current_date\n",
        "                and event = 'Login'\n",
        "                group by 1,2'''},\n",
        "\n",
        "{\n",
        "    \"question\":\"find storewise total user searches in the last 30 days\",\n",
        "    \"sql\":'''     select cast(timestamp as date) as date,\n",
        "                store,\n",
        "                count(distinct email) as Searches\n",
        "\n",
        "                from events\n",
        "                where (source_name is null) or  (source_name in ('web','app'))\n",
        "                and cast(timestamp as date) between current_date-30 and current_date\n",
        "                and event = 'Search Term'\n",
        "                group by 1,2'''},\n",
        "\n",
        "{\n",
        "    \"question\":\"find storewise total users who viewed collections in the last 30 days\",\n",
        "    \"sql\":'''     select cast(timestamp as date) as date,\n",
        "                store,\n",
        "                count(distinct email) as count_collection_views\n",
        "\n",
        "                from events\n",
        "                where (source_name is null) or  (source_name in ('web','app'))\n",
        "                and cast(timestamp as date) between current_date-30 and current_date\n",
        "                and event = 'Collection Viewed'\n",
        "                group by 1,2'''},\n",
        "\n",
        "{\n",
        "    \"question\":\"find storewise product views in the last 30 days\",\n",
        "    \"sql\":'''     select cast(timestamp as date) as date,\n",
        "                store,\n",
        "                count(distinct email) as count_product_views\n",
        "\n",
        "                from events\n",
        "                where (source_name is null) or  (source_name in ('web','app'))\n",
        "                and cast(timestamp as date) between current_date-30 and current_date\n",
        "                and event IN ('Viewed Product','Product Viewed')\n",
        "                group by 1,2'''},\n",
        "\n",
        "\n",
        "{\n",
        "    \"question\":\"find storewise product views for the last 30 days\",\n",
        "    \"sql\":'''     select cast(timestamp as date) as date,\n",
        "                store,\n",
        "                count(distinct email) as orders_placed\n",
        "\n",
        "                from events\n",
        "\n",
        "                where (source_name is null) or  (source_name in ('web','app'))\n",
        "                and cast(timestamp as date) between current_date-30 and current_date\n",
        "                and event IN ('Placed Order')\n",
        "                group by 1,2'''},\n",
        "\n",
        "{\n",
        "    \"question\":\"find storwise total user logins in last 30 days\",\n",
        "    \"sql\":'''     with base as\n",
        "                (select cast(timestamp as date) as date,\n",
        "                store,\n",
        "                count(distinct email) as login\n",
        "\n",
        "                from events\n",
        "\n",
        "                where (source_name is null) or  (source_name in ('web','app'))\n",
        "                and cast(timestamp as date) between current_date-30 and current_date\n",
        "                and event = 'Login'\n",
        "                group by 1,2)\n",
        "\n",
        "                select store, sum(login) as total_logins_30_days\n",
        "                from base\n",
        "                group by 1'''},\n",
        "\n",
        "{\n",
        "    \"question\":\"find storwise total user logins in last 30 days\",\n",
        "    \"sql\":'''     with base as\n",
        "                (select cast(timestamp as date) as date,\n",
        "                store,\n",
        "                count(distinct email) as login\n",
        "\n",
        "                from events\n",
        "\n",
        "                where (source_name is null) or  (source_name in ('web','app'))\n",
        "                and cast(timestamp as date) between current_date-30 and current_date\n",
        "                and event = 'Login'\n",
        "                group by 1,2)\n",
        "\n",
        "                select store, sum(login) as total_logins_30_days\n",
        "                from base\n",
        "                group by 1'''},\n",
        "\n",
        "{\n",
        "    \"question\":\"can you find the store and source wise cart penetration for different product types for the last 30 days?\",\n",
        "    \"sql\": '''\n",
        "\n",
        "    with ptype_occurences as\n",
        "    (select store, source_name, product_type, id\n",
        "    from order_item\n",
        "    where cast(created_at as date) between current_date-30 and current_date\n",
        "    group by 1,2,3,4),\n",
        "\n",
        "    totals as\n",
        "    (select store, source_name, count(distinct id) as total_orders\n",
        "    from order_item\n",
        "    where cast(created_at as date) between current_date-30 and current_date\n",
        "    group by 1,2)\n",
        "\n",
        "    select a.store, a.source_name, a.product_type, count(distinct a.id)*100.00/b.total_orders as occur_cart_pen\n",
        "    from ptype_occurences a\n",
        "    left join totals b on a.store = b.store and a.source_name = b.source_name\n",
        "    group by 1,2,3,b.total_orders\n",
        "\n",
        "    '''},\n",
        "\n",
        "{\n",
        "    \"question\":\"What is the average store and source wise month on month retention%?\",\n",
        "    \"sql\": '''\n",
        "\n",
        "    with base as (\n",
        "        select store ,(case when source_name = 'pos' then 'pos' else 'online' end) as source , date_trunc('MONTH',created_at)::DATE as month , email from orders group by 1,2,3,4\n",
        "    ) ,\n",
        "\n",
        "     ret_base as (\n",
        "        select a.store , a.source , a.month as base_month, b.month as retained_month , count(distinct b.email) as retained_users\n",
        "        from base a\n",
        "        join base b on b.month >= a.month and b.email = a.email\n",
        "        group by 1,2,3,4\n",
        "     ),\n",
        "\n",
        "     totals_base as (\n",
        "      select a.store, a.source, a.month as base_month, count(distinct a.email) as total_users\n",
        "      from base a\n",
        "      group by 1,2,3)\n",
        "\n",
        "    select store, source, avg(1.00*percentage_retained_users) as avg_percentage_retained_users\n",
        "\n",
        "    from\n",
        "\n",
        "    (select a.store, a.source, a.base_month, a.retained_month, a.retained_users*100.00/b.total_users as percentage_retained_users\n",
        "    from ret_base a\n",
        "    left join totals_base b on a.store=b.store and a.source=b.source and a.base_month=b.base_month) a\n",
        "\n",
        "    group by 1,2\n",
        "\n",
        "    '''},\n",
        "\n",
        "{\n",
        "        \"question\":\"Can you provide the breakdown of interacted users by store?\",\n",
        "        \"sql\":'''\n",
        "\n",
        "        SELECT store, COUNT(DISTINCT email) AS interacted_users\n",
        "        FROM events\n",
        "        GROUP BY store'''},\n",
        "\n",
        "{\n",
        "        \"question\":\"Can you provide the breakdown of interacted users by store for each event type?\",\n",
        "        \"sql\":'''\n",
        "\n",
        "        select store, event, count(distinct email) as interacted_users\n",
        "        from events\n",
        "        group by 1,2'''},\n",
        "\n",
        "{\n",
        "        \"question\":\"What are the top three stores with the highest number of interacted users?\",\n",
        "        \"sql\":'''\n",
        "        WITH event_counts AS (\n",
        "    SELECT store,\n",
        "           COUNT(DISTINCT email) AS interacted_users\n",
        "    FROM events\n",
        "    WHERE source_name IN ('web', 'app')\n",
        "      AND timestamp::DATE BETWEEN CURRENT_DATE - 30 AND CURRENT_DATE\n",
        "    GROUP BY store\n",
        "    )\n",
        "\n",
        "    SELECT store, interacted_users\n",
        "    FROM event_counts\n",
        "    ORDER BY interacted_users DESC\n",
        "    LIMIT 3'''},\n",
        "\n",
        "{\n",
        "        \"question\":\"What are the top 10 selling items in the last 30 days?\",\n",
        "        \"sql\":'''with base as\n",
        "        (select title_x as product_name, sum(item_selling_price::decimal) as total_sales\n",
        "        from order_item\n",
        "        where created_at::date between current_date-30 and current_date\n",
        "        group by 1\n",
        "        order by total_sales desc)\n",
        "\n",
        "        select * from base\n",
        "        limit 10'''},\n",
        "\n",
        "{\n",
        "        \"question\":\"What are the store-wise top 10 selling items in the last 30 days?\",\n",
        "        \"sql\":'''with base as\n",
        "        (select store, title_x as product_name, sum(item_selling_price::decimal) as total_sales,\n",
        "        rank() OVER(PARTITION BY store order by sum(item_selling_price::decimal) desc) as sales_rank\n",
        "        from order_item\n",
        "        where created_at::date between current_date-30 and current_date\n",
        "        group by 1,2)\n",
        "\n",
        "        select store, product_name, total_sales, sales_rank\n",
        "        from base\n",
        "        where sales_rank<=10\n",
        "        order by sales_rank, store'''},\n",
        "\n",
        "{\n",
        "        \"question\":\"Which store has the highest sales in the last 30 days?\",\n",
        "        \"sql\":'''SELECT store, SUM(total_price) AS total_sales\n",
        "                FROM orders\n",
        "                WHERE created_at >= current_date - interval '30 days'\n",
        "                GROUP BY store\n",
        "                ORDER BY total_sales DESC\n",
        "                LIMIT 1'''},\n",
        "\n",
        "{\n",
        "        \"question\":\"What is the distribution of sales by day of the week for the highest-selling store in the last 30 days?\",\n",
        "        \"sql\":'''with highest_selling_store as (\n",
        "        select store\n",
        "        from orders\n",
        "        where date(created_at) between current_date - 30 and current_date\n",
        "        group by store\n",
        "        order by sum(total_price) desc\n",
        "        limit 1\n",
        "    )\n",
        "\n",
        "    select extract(dow from o.created_at::timestamp) as day_of_week,\n",
        "           sum(o.total_price) as total_sales\n",
        "    from orders o\n",
        "    where o.store = (select * from highest_selling_store)\n",
        "      and date(o.created_at) between current_date - 30 and current_date\n",
        "    group by day_of_week\n",
        "    order by day_of_week;'''},\n",
        "\n",
        "{\n",
        "        \"question\":\"What percentage of total sales in the last 30 days for the highest-selling store came from repeat customers?\",\n",
        "        \"sql\":'''WITH total_sales AS (\n",
        "    SELECT store, SUM(total_price) AS total_sales\n",
        "    FROM orders\n",
        "    WHERE created_at >= current_date - INTERVAL '30 days'\n",
        "    GROUP BY store\n",
        "),\n",
        "repeat_customer_sales AS (\n",
        "    SELECT store, SUM(total_price) AS repeat_sales\n",
        "    FROM orders\n",
        "    WHERE created_at >= current_date - INTERVAL '30 days'\n",
        "    AND email IN (\n",
        "        SELECT email\n",
        "        FROM orders\n",
        "        WHERE created_at < current_date - INTERVAL '30 days'\n",
        "    )\n",
        "    GROUP BY store\n",
        "),\n",
        "highest_selling_store AS (\n",
        "    SELECT store\n",
        "    FROM total_sales\n",
        "    ORDER BY total_sales DESC\n",
        "    LIMIT 1\n",
        ")\n",
        "\n",
        "SELECT t.store,\n",
        "    (r.repeat_sales / t.total_sales) * 100 AS repeat_customer_percentage\n",
        "FROM total_sales t\n",
        "JOIN repeat_customer_sales r ON t.store = r.store\n",
        "CROSS JOIN highest_selling_store; '''},\n",
        "\n",
        "{\n",
        "        \"question\":\"Can you identify the top 10 products that are frequently purchased by repeat customers in the last 30 days for each store?\",\n",
        "        \"sql\":'''\n",
        "with repeat_customer_sales AS (\n",
        "    SELECT store, title_x as product_name, SUM(item_selling_price::float) AS repeat_sales,\n",
        "    rank() OVER(PARTITION BY store order by SUM(item_selling_price::float) desc) as product_rank\n",
        "    FROM order_item\n",
        "    WHERE created_at >= current_date - INTERVAL '30 days'\n",
        "    AND email IN (\n",
        "        SELECT email\n",
        "        FROM orders\n",
        "        WHERE created_at < current_date - INTERVAL '30 days'\n",
        "    )\n",
        "    GROUP BY store, title_x)\n",
        "\n",
        "    select store, product_name, repeat_sales, product_rank\n",
        "    from repeat_customer_sales\n",
        "    where product_rank<=10\n",
        "    order by product_rank, store\n",
        "\n",
        "    '''},\n",
        "\n",
        "{\n",
        "        \"question\":\"How do user engagement metrics differ between weekdays and weekends?\",\n",
        "        \"sql\":'''with engagement_metrics as (\n",
        "        select\n",
        "            store,\n",
        "            case\n",
        "                when extract(dow from timestamp::date) in (0, 6) then 'Weekend'\n",
        "                else 'Weekday'\n",
        "            end as day_type,\n",
        "            count(distinct case when event = 'Login' then email end) as logins,\n",
        "            count(distinct case when event = 'Search Term' then email end) as searches,\n",
        "            count(distinct case when event = 'Collection Viewed' then email end) as collections_viewed,\n",
        "            count(distinct case when event in ('Viewed Product', 'Product Viewed') then email end) as products_viewed,\n",
        "            count(distinct case when event = 'Added To Cart' then email end) as added_to_cart,\n",
        "            count(distinct case when event = 'Placed Order' then email end) as orders_placed\n",
        "        from events\n",
        "        group by 1, 2\n",
        "    )\n",
        "    select\n",
        "        store,\n",
        "        day_type,\n",
        "        avg(logins) as avg_logins,\n",
        "        avg(searches) as avg_searches,\n",
        "        avg(collections_viewed) as avg_collections_viewed,\n",
        "        avg(products_viewed) as avg_products_viewed,\n",
        "        avg(added_to_cart) as avg_added_to_cart,\n",
        "        avg(orders_placed) as avg_orders_placed\n",
        "    from engagement_metrics\n",
        "    group by 1, 2'''},\n",
        "\n",
        "{\"question\":\"What is the average order value for each product type in the last 30 days?\",\n",
        "        \"sql\":'''select product_type, avg(item_selling_price::float) as avg_order_value\n",
        "    from order_item\n",
        "    where date(created_at) between current_date-30 and current_date\n",
        "    group by product_type\n",
        "\n",
        "    '''},\n",
        "\n",
        "{\"question\":\"is there a trend for discount and weekdays?\",\n",
        "        \"sql\":'''WITH discount_trend AS (\n",
        "    SELECT\n",
        "        EXTRACT(DOW FROM created_at::date) AS day_of_week,\n",
        "        id,\n",
        "        sum(coalesce(discount::float,0)) AS order_discount\n",
        "    FROM order_item\n",
        "    GROUP BY day_of_week, id\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    day_of_week,\n",
        "    avg(order_discount) as avg_discount\n",
        "FROM discount_trend\n",
        "group by day_of_week\n",
        "ORDER BY day_of_week\n",
        "\n",
        "    '''},\n",
        "\n",
        "{\"question\":\"what are the top discounted product categories?\",\n",
        "        \"sql\":'''\n",
        "with base as(\n",
        "    SELECT product_type, id, SUM(discount::float) AS total_discount\n",
        "FROM order_item\n",
        "GROUP BY product_type, id)\n",
        "\n",
        "select product_type, avg(total_discount) as avg_total_discount\n",
        "from base\n",
        "group by 1\n",
        "ORDER BY avg_total_discount DESC\n",
        "\n",
        "    '''},\n",
        "\n",
        "{\"question\":\"does more discount mean more orders?\",\n",
        "        \"sql\":'''\n",
        "with base as(\n",
        "    SELECT created_at::date as date_, id, SUM(discount::float) AS total_discount\n",
        "FROM order_item\n",
        "GROUP BY 1, id)\n",
        "\n",
        "select date_, avg(total_discount) as avg_total_discount\n",
        "from base\n",
        "group by 1\n",
        "ORDER BY 1\n",
        "\n",
        "    '''},]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gJqpbeTw0yqn"
      },
      "outputs": [],
      "source": [
        "ddl = [\n",
        "\n",
        "    \"\"\"CREATE TABLE orders (\n",
        "    id INT PRIMARY KEY,\n",
        "    created_at TIMESTAMP,\n",
        "    line_items JSON,\n",
        "    store VARCHAR(255),\n",
        "    email VARCHAR(255),\n",
        "    shipping_lines JSON,\n",
        "    source_name VARCHAR(255),\n",
        "    source_identifier VARCHAR(255),\n",
        "    total_line_items_price DECIMAL(10, 2),\n",
        "    total_price DECIMAL(10, 2),\n",
        "    total_discounts DECIMAL(10, 2)\n",
        "    );\"\"\"  ,\n",
        "\n",
        "    \"\"\"CREATE TABLE order_item (\n",
        "    id INT PRIMARY KEY,\n",
        "    created_at TIMESTAMP,\n",
        "    store VARCHAR(255),\n",
        "    email VARCHAR(255),\n",
        "    source_name VARCHAR(255),\n",
        "    source_identifier VARCHAR(255),\n",
        "    sku VARCHAR(255),\n",
        "    quantity INT,\n",
        "    title_x VARCHAR(255),\n",
        "    date DATE,\n",
        "    product_type VARCHAR(255),\n",
        "    vendor VARCHAR(255),\n",
        "    weight DECIMAL(10, 2),\n",
        "    weight_unit VARCHAR(50),\n",
        "    item_selling_price DECIMAL(10, 2),\n",
        "    discount DECIMAL(5, 2)\n",
        "    );\"\"\" ,\n",
        "\n",
        "    \"\"\"CREATE TABLE events (\n",
        "    id INT PRIMARY KEY,\n",
        "    timestamp VARCHAR(255),\n",
        "    event VARCHAR(255),\n",
        "    sku VARCHAR(255),\n",
        "    product_name VARCHAR(255),\n",
        "    store VARCHAR(255),\n",
        "    email VARCHAR(255),\n",
        "    source_name VARCHAR(255)\n",
        "    );\"\"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qLgYrLGT03On"
      },
      "outputs": [],
      "source": [
        "docs = [\n",
        "\"If similar question present in example straightforward use that query. \" ,\n",
        "\"Week is calculated as : date_Trunc('week' , created_at)::date as week\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_Ubu98B2qPrU"
      },
      "outputs": [],
      "source": [
        "benchmark = [\n",
        "{'metric' : 'order'  , 'value': 500} ,{'metric' : 'retention'  , 'value':  15} ,{'metric' : 'aov'  , 'value':  70} ,{'metric' : 'churn'  , 'value':  75} ,{'metric' : 'conversion'  , 'value':  10} ,\n",
        "{'metric' : 'sales'  , 'value': 50000}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GGNvxCrpXKLb"
      },
      "outputs": [],
      "source": [
        "##############################################################################  Utility Functions   #############################################################################################\n",
        "\n",
        "# Function to extract important aspects from the question\n",
        "def extract_details(question):\n",
        "    # Load the English language model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    doc = nlp(question.lower())  # Convert to lowercase for case-insensitive matching\n",
        "    actions = ['select', 'count', 'sum', 'average', 'total']  # Common SQL actions\n",
        "    entities = []\n",
        "    action = None\n",
        "\n",
        "    analytics_words = ['retention', 'churn', 'conversion', 'growth', 'engagement', 'acquisition', 'orders', 'penetration', 'sales','interaction', 'event' ,\n",
        "    'logins', 'searched', 'collection_viewed', 'product_viewed', 'atc' , 'new' ,'existing' ,'product views' ,'storewise' ,'store' ,'source' ,'user logins' , 'interacted users' ,'top selling' ,\n",
        "    'average' , 'median' , 'month on month' ,'time' , 'first' , 'second' , 'third' , 'fourth' ,'fifth']\n",
        "\n",
        "    threshold = 90\n",
        "\n",
        "    # Extract entities and action from the question\n",
        "    for token in doc:\n",
        "        # if token.text in actions:\n",
        "        #     entities.append(token.text)\n",
        "        # elif token.pos_ == 'NUM' or token.text.isdigit():  # Include digits as numbers\n",
        "        #     entities.append(token.text)\n",
        "        # elif token.pos_ == 'NOUN':  # Exclude 'days' from entities\n",
        "        #     entities.append(token.text)\n",
        "        # else:\n",
        "            # Check for similarity with analytics words\n",
        "        matches = process.extractOne(token.text, analytics_words)\n",
        "        if matches[1] > threshold:\n",
        "          entities.append(matches[0])\n",
        "\n",
        "    return \" \".join(entities)\n",
        "\n",
        "\n",
        "#######################################\n",
        "\n",
        "def modify_data(dataset):\n",
        "\n",
        "    # Modify the dataset to include the concatenated important aspects\n",
        "    dataset_with_concatenated_aspects = []\n",
        "    for data in dataset:\n",
        "        question = data[\"question\"]\n",
        "        concatenated_aspects = extract_details(question)\n",
        "        data_with_concatenated_aspects = data.copy()  # Make a copy of the original data\n",
        "        data_with_concatenated_aspects[\"important_aspects\"] = concatenated_aspects  # Add the concatenated aspects\n",
        "        dataset_with_concatenated_aspects.append(data_with_concatenated_aspects)\n",
        "\n",
        "    # Print the modified dataset with important aspects concatenated into a single string\n",
        "    important_aspects=[entry[\"important_aspects\"] for entry in dataset_with_concatenated_aspects]\n",
        "    question_texts = [entry[\"important_aspects\"] for entry in dataset_with_concatenated_aspects]\n",
        "    sql = [{\"sql\": entry[\"sql\"],\n",
        "        \"question\": entry[\"question\"]} for entry in dataset_with_concatenated_aspects]\n",
        "    return question_texts,sql\n",
        "\n",
        "\n",
        "\n",
        "##########################################\n",
        "\n",
        "def execute_query(sql_query):\n",
        "    password_postgres = ''\n",
        "    username_postgres = ''\n",
        "    connection_string = ''\n",
        "    engine = create_engine(connection_string)\n",
        "    df = pd.DataFrame()\n",
        "    with engine.connect() as conn, conn.begin():\n",
        "        df = pd.read_sql(sql_query, conn)\n",
        "        conn.close()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "def check_question_db(question , data_sql):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    lower_question = nlp(question.lower())\n",
        "    matched_sql = None\n",
        "    max_match = 0\n",
        "    for i in data_sql['metadatas']:\n",
        "        for k in i:\n",
        "            q = nlp(k['question'].lower())\n",
        "            f = fuzz.token_set_ratio(q , lower_question)\n",
        "            if f >= 75 and extract_details(k['question']) == extract_details(question):\n",
        "                if f > max_match:\n",
        "                    matched_sql = k['sql']\n",
        "                    max_match = f\n",
        "\n",
        "\n",
        "    return matched_sql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Wu7iU6aaL-Z1"
      },
      "outputs": [],
      "source": [
        "##################################################################  ChromaDB class  ##############################################################################\n",
        "class chroma_db:\n",
        "\n",
        "    def __init__(self , client_type = \"temporary\"):\n",
        "        if client_type == \"temporary\":\n",
        "            self.client = chromadb.Client()\n",
        "        else:\n",
        "            self.client = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\",\n",
        "                                    persist_directory=\"db/\"\n",
        "                                ))\n",
        "        self.collection_sql = self.client.get_or_create_collection(name = \"sql\")\n",
        "        self.collection_ddl = self.client.get_or_create_collection(name = \"ddl\")\n",
        "        self.collection_docs = self.client.get_or_create_collection(name = \"docs\")\n",
        "        self.collection_benchmark = self.client.get_or_create_collection(name = \"benchmark\")\n",
        "        self.embedding_function = embedding_functions.DefaultEmbeddingFunction()\n",
        "\n",
        "\n",
        "\n",
        "    def generate_embedding(self, data, **kwargs) -> List[float]:\n",
        "        embedding = self.embedding_function(data)\n",
        "        if len(embedding) == 1:\n",
        "            return embedding[0]\n",
        "        return embedding\n",
        "\n",
        "\n",
        "    def add_sql(self, question_texts , sql):\n",
        "        ids =  [str(i) for i in range(len(question_texts))]\n",
        "        self.collection_sql.upsert(\n",
        "            documents = question_texts,\n",
        "            metadatas = sql,\n",
        "            ids = ids\n",
        "        )\n",
        "\n",
        "\n",
        "    def add_ddl(self, ddl ):\n",
        "        ids = [str(i) for i in range(len(ddl))]\n",
        "        self.collection_ddl.upsert(\n",
        "            documents=ddl,\n",
        "            #embeddings=self.generate_embedding(ddl),\n",
        "            ids=ids ,\n",
        "        )\n",
        "\n",
        "\n",
        "    def add_docs(self, docs ):\n",
        "        ids = [str(i) for i in range(len(docs))]\n",
        "        self.collection_docs.upsert(\n",
        "            documents=docs,\n",
        "           # embeddings=self.generate_embedding(docs),\n",
        "            ids=ids,\n",
        "        )\n",
        "\n",
        "    def add_benchmark(self,benchmark):\n",
        "        ids = [str(i) for i in range(len(benchmark))]\n",
        "        docs = [dic['metric'] for dic in benchmark]\n",
        "        self.collection_benchmark.upsert(\n",
        "            documents = docs,\n",
        "            metadatas = benchmark,\n",
        "            ids = ids\n",
        "        )\n",
        "\n",
        "    def view_sql_db(self):\n",
        "        return self.collection_sql.get()\n",
        "\n",
        "    def view_ddl_db(self):\n",
        "        return self.collection_ddl.get()\n",
        "\n",
        "    def view_docs_db(self):\n",
        "        return self.collection_docs.get()\n",
        "\n",
        "    def view_benchmark_db(self):\n",
        "        return self.collection_benchmark.get()\n",
        "\n",
        "\n",
        "    def update_data(self , ids , metadatas , documents):\n",
        "        self.collection.update(ids = ids , documents = documents , metadatas = metadatas)\n",
        "\n",
        "    def delete(self , ids):\n",
        "        self.delete(ids = ids)\n",
        "\n",
        "    def retrieve_sql_data(self , question , n_results = 3):\n",
        "        ques_details = extract_details(question)\n",
        "        data = self.collection_sql.query(\n",
        "        query_texts=[ques_details],\n",
        "        n_results=n_results\n",
        "        )\n",
        "        return data\n",
        "\n",
        "    def retrieve_ddl_data(self , question ,n_results = 3):\n",
        "        #ques_details = extract_details(question)\n",
        "        data = self.collection_ddl.query(\n",
        "        query_texts=[question],\n",
        "        n_results=n_results\n",
        "        )\n",
        "        return data\n",
        "\n",
        "    def retrieve_docs_data(self , question ,n_results = 3):\n",
        "        #ques_details = extract_details(question)\n",
        "        data = self.collection_docs.query(\n",
        "        query_texts=[question],\n",
        "        n_results=n_results\n",
        "        )\n",
        "        return data\n",
        "\n",
        "    def retrieve_benchmark_data(self , question ,n_results = 1):\n",
        "        ques_details = extract_details(question)\n",
        "        data = self.collection_benchmark.query(\n",
        "        query_texts=[ques_details],\n",
        "        n_results=n_results\n",
        "        )\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YvdC1nd7XoMs"
      },
      "outputs": [],
      "source": [
        "#######################################################################  LLM model class  #####################################################################################\n",
        "class customllm:\n",
        "    def __init__(self , api_key : str , model : str , base_url : str):\n",
        "        #self.client = OpenAI(api_key = api_key)\n",
        "        self.client = OpenAI(base_url = base_url , api_key = api_key )\n",
        "        self.model = model\n",
        "\n",
        "    def system_message(self , message: str) -> any:\n",
        "        return {\"role\": \"system\", \"content\": message}\n",
        "\n",
        "\n",
        "    def user_message(self , message: str) -> any:\n",
        "        return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "\n",
        "    def assistant_message(self , message: str) -> any:\n",
        "        return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "    def ask(self , question , sql_data , ddl_data , docs_data , temperature = 0.3):\n",
        "        prompt = \"\"\n",
        "        prompt = \"[Instruction]You are a SQL Query writer.The user provides a question and you provide SQL.Respond with only SQL code. Do not answer with any explanations -- just the code.\\n\"\n",
        "\n",
        "        prompt += f\"You should always refer this definitions:\"\n",
        "        docs = [doc for doc_list in docs_data['documents'] for doc in doc_list]\n",
        "\n",
        "        for i in range(len(docs)):\n",
        "            prompt = prompt + docs[i] + \"\\n\"\n",
        "\n",
        "\n",
        "        prompt += f\"\\nYou may use the following DDL statements as a reference for what tables might be available:\"\n",
        "        ddl = [doc for doc_list in ddl_data['documents'] for doc in doc_list]\n",
        "\n",
        "        for i in range(len(ddl)):\n",
        "            prompt = prompt + ddl[i] + \"[~Instruction]\\n \"\n",
        "\n",
        "        message_log = [self.system_message(prompt)]\n",
        "\n",
        "        prompt = prompt + \"\\n You have these set of example questions and queries:\\n\"\n",
        "\n",
        "        # for result in results:\n",
        "        sql_d = [meta['sql'] for metadata_list in sql_data['metadatas'] for meta in metadata_list]\n",
        "        documents = [doc for doc_list in sql_data['documents'] for doc in doc_list]\n",
        "        questions = [meta['question'] for metadata_list in sql_data['metadatas'] for meta in metadata_list]\n",
        "\n",
        "        num_tokens = 0\n",
        "        for i in range(len(sql_d)):\n",
        "            for message in message_log:\n",
        "                num_tokens += len(message[\"content\"]) / 4\n",
        "            if num_tokens <= 900:\n",
        "                message_log.append(self.user_message(questions[i]))\n",
        "                message_log.append(self.assistant_message(sql_d[i]))\n",
        "            #prompt = prompt + \"question:\" + questions[i] + \"\\nsql:\" + sql_d[i] + \"\\n\"\n",
        "\n",
        "        message_log.append(self.user_message(question))\n",
        "        return message_log\n",
        "\n",
        "\n",
        "    def submit_prompt(self, prompt, temperature = 0.2 , max_tokens = 3000) -> str:\n",
        "        if prompt is None:\n",
        "            raise Exception(\"Prompt is None\")\n",
        "\n",
        "        if len(prompt) == 0:\n",
        "            raise Exception(\"Prompt is empty\")\n",
        "\n",
        "        # Count the number of tokens in the message log\n",
        "        # Use 4 as an approximation for the number of characters per token\n",
        "        num_tokens = 0\n",
        "        for message in prompt:\n",
        "            num_tokens += len(message[\"content\"]) / 4\n",
        "\n",
        "        response =  self.client.chat.completions.create(\n",
        "              model= self.model ,\n",
        "              messages= prompt ,\n",
        "              temperature= temperature,\n",
        "              max_tokens= max(max_tokens , num_tokens)\n",
        "              )\n",
        "\n",
        "        query = response.choices[0].message.content\n",
        "\n",
        "        for choice in response.choices:\n",
        "            if \"text\" in choice:\n",
        "                query = choice.text\n",
        "\n",
        "        sql = re.search(r\"```sql\\n(.*)```\", query, re.DOTALL)\n",
        "        if sql:\n",
        "            #self.log(f\"Output from LLM: {query} \\nExtracted SQL: {sql.group(1)}\")\n",
        "            query =  sql.group(1)\n",
        "\n",
        "        sql = re.search(r\"```(.*)```\", query, re.DOTALL)\n",
        "        if sql:\n",
        "            #self.log(f\"Output from LLM: {query} \\nExtracted SQL: {sql.group(1)}\")\n",
        "            query = sql.group(1)\n",
        "\n",
        "        query = query.replace(\"\\_\" , \"_\")\n",
        "        query = query.replace(\"\\\\\" , \"\")\n",
        "\n",
        "        return query\n",
        "\n",
        "    def generate_summary(self, question: str, df: pd.DataFrame, temperature  ,benchmark , max_tokens , **kwargs) -> str:\n",
        "        message_log = [\n",
        "            self.system_message(\n",
        "                f\"You are a great Sherlock Holmes of Data. The user asked the question: '{question}'\\n\\nThe following is a pandas DataFrame with the results of the query: \\n{df.to_markdown()}\\n\\n\"\n",
        "            ),\n",
        "            self.user_message(\n",
        "                f\"[Instruction]Generate great insights from the data.Use benchmark numbers from following dictionary:{benchmark['metadatas']}\\n.Don't output the whole same data and obvious facts. Do not respond with any additional explanation beyond the summary.[~Instruction]\"\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        summary = self.submit_prompt(prompt = message_log,temperature = temperature ,max_tokens = max_tokens , **kwargs)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def regenerate_query(self , prompt : any , error ,query , temperature = 0.1 , **kwargs):\n",
        "        additions = []\n",
        "        additions.append(self.system_message(f\"You are a great SQL debugger and respond with only SQL query-no explanation and the user got the error with following SQL query : {query}\\n\\n\"))\n",
        "        additions.append(self.user_message(f\"[Instruction]Please correct this error :{error} \\n in the SQL query generated and output correct SQL query[~Instruction]\\n\\n\"))\n",
        "        new_prompt =  additions\n",
        "        sql_query = self.submit_prompt(prompt = new_prompt , temperature = 0.1)\n",
        "        return sql_query\n",
        "\n",
        "\n",
        "    def _extract_python_code(self, markdown_string: str) -> str:\n",
        "        # Regex pattern to match Python code blocks\n",
        "        pattern = r\"```[\\w\\s]*python\\n([\\s\\S]*?)```|```([\\s\\S]*?)```\"\n",
        "\n",
        "        # Find all matches in the markdown string\n",
        "        matches = re.findall(pattern, markdown_string, re.IGNORECASE)\n",
        "\n",
        "        # Extract the Python code from the matches\n",
        "        python_code = []\n",
        "        for match in matches:\n",
        "            python = match[0] if match[0] else match[1]\n",
        "            python_code.append(python.strip())\n",
        "\n",
        "        if len(python_code) == 0:\n",
        "            return markdown_string\n",
        "\n",
        "        return python_code[0]\n",
        "\n",
        "    def _sanitize_plotly_code(self, raw_plotly_code: str) -> str:\n",
        "        # Remove the fig.show() statement from the plotly code\n",
        "        plotly_code = raw_plotly_code.replace(\"fig.show()\", \"\")\n",
        "\n",
        "        return plotly_code\n",
        "\n",
        "\n",
        "\n",
        "    def generate_plotly_code(\n",
        "        self, question: str = None, sql: str = None, df_metadata: str = None, temperature = 0.3 , **kwargs) -> str:\n",
        "        if question is not None:\n",
        "            system_msg = f\"The following is a pandas DataFrame that contains the results of the query that answers the question the user asked: '{question}'\"\n",
        "        else:\n",
        "            system_msg = \"The following is a pandas DataFrame \"\n",
        "\n",
        "        if sql is not None:\n",
        "            system_msg += f\"\\n\\nThe DataFrame was produced using this query: {sql}\\n\\n\"\n",
        "\n",
        "        system_msg += f\"The following is information about the resulting pandas DataFrame 'df': \\n{df_metadata}\"\n",
        "\n",
        "        message_log = [\n",
        "            self.system_message(system_msg),\n",
        "            self.user_message(\n",
        "                \"Can you generate the Python plotly code to chart the results of the dataframe? Assume the data is in a pandas dataframe called 'df'. If there is only one value in the dataframe, use an Indicator. Respond with only Python code. Do not answer with any explanations -- just the code.\"\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        plotly_code = self.submit_prompt(message_log , temperature = temperature)\n",
        "\n",
        "        return self._sanitize_plotly_code(self._extract_python_code(plotly_code))\n",
        "\n",
        "    def get_plotly_figure(\n",
        "        self, plotly_code: str, df: pd.DataFrame, dark_mode: bool = True) -> plotly.graph_objs.Figure:\n",
        "        ldict = {\"df\": df, \"px\": px, \"go\": go}\n",
        "        try:\n",
        "            exec(plotly_code, globals(), ldict)\n",
        "\n",
        "            fig = ldict.get(\"fig\", None)\n",
        "        except:\n",
        "                # Inspect data types\n",
        "            fig = None\n",
        "            date_cols = df.select_dtypes(include=[\"datetime\"]).columns.tolist()\n",
        "            numeric_cols = df.select_dtypes(include=[\"number\" ,\"int\" ,\"float\"]).columns.tolist()\n",
        "            categorical_cols = df.select_dtypes(\n",
        "                include=[\"object\", \"category\"]\n",
        "            ).columns.tolist()\n",
        "\n",
        "                # Decision-making for plot type\n",
        "\n",
        "            if len(date_cols) + len(categorical_cols) <= 3 and len(date_cols) + len(categorical_cols) > 0:\n",
        "                figure = []\n",
        "                figure_traces = []\n",
        "\n",
        "                if len(date_cols) == 0:\n",
        "                    date_cols = []\n",
        "                    date_cols.append(categorical_cols[0])\n",
        "                    categorical_cols.pop(0)\n",
        "                for i in range(len(numeric_cols)):\n",
        "                    if len(categorical_cols) == 0:\n",
        "                        figure.append(px.bar(df , x = date_cols[0] , y = numeric_cols[i] ,text = numeric_cols[i]))\n",
        "                    elif len(categorical_cols) == 1:\n",
        "                        figure.append(px.bar(df , x = date_cols[0] , y = numeric_cols[i] , color = categorical_cols[0],pattern_shape= categorical_cols[0] , text = numeric_cols[i]))\n",
        "                    else:\n",
        "                        figure.append(px.bar(df , x = date_cols[0] , y = numeric_cols[i] , color = categorical_cols[0] , pattern_shape= categorical_cols[1] , text = numeric_cols[i]))\n",
        "\n",
        "\n",
        "                # For as many traces that exist per Express figure, get the traces from each plot and store them in an array.\n",
        "                # This is essentially breaking down the Express fig into it's traces\n",
        "\n",
        "                for i in range(len(numeric_cols)):\n",
        "                    figure_traces.append([])\n",
        "                    for trace in range(len(figure[i][\"data\"])):\n",
        "                        figure_traces[i].append(figure[i][\"data\"][trace])\n",
        "\n",
        "\n",
        "                #Create a 1x2 subplot\n",
        "                fig = sp.make_subplots(rows=1, cols=len(numeric_cols))\n",
        "\n",
        "                for i in range(1,len(numeric_cols)+1):\n",
        "                    fig['layout']['xaxis{}'.format(i)]['title']= date_cols[0]\n",
        "                    fig['layout']['yaxis{}'.format(i)]['title']= numeric_cols[i-1]\n",
        "\n",
        "                # Get the Express fig broken down as traces and add the traces to the proper plot within in the subplot\n",
        "                for i in range(1 ,len(numeric_cols)+1):\n",
        "                    for traces in figure_traces[i-1]:\n",
        "                        fig.append_trace(traces, row=1, col=i)\n",
        "\n",
        "            else:\n",
        "                # Default to a simple line plot if above conditions are not met\n",
        "                fig = px.line(df)\n",
        "\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def generate_followup_questions(self, question: str, sql: str, df: pd.DataFrame, **kwargs ) -> list:\n",
        "        message_log = [\n",
        "            self.system_message(\n",
        "                f\"You are a helpful data assistant. The user asked the question: '{question}'\\n\\nThe SQL query for this question was: {sql}\\n\\nThe following is a pandas DataFrame with the results of the query: \\n{df.to_markdown()}\\n\\n\"\n",
        "            ),\n",
        "            self.user_message(\n",
        "                \"[Instruction]Generate a list of three followup questions that the user might ask about this data. Respond with a list of questions, one per line. Do not answer with any explanations -- just the questions.[~Instruction]\"\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "        llm_response = self.submit_prompt(message_log, **kwargs)\n",
        "\n",
        "        numbers_removed = re.sub(r\"^\\d+\\.\\s*\", \"\", llm_response, flags=re.MULTILINE)\n",
        "        l = numbers_removed.split(\"\\n\")\n",
        "        return l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R_IBDvxBACxz"
      },
      "outputs": [],
      "source": [
        "##########################################################################################  Main run  ################################################################################################\n",
        "#running the script\n",
        "def run(question , my_db : chroma_db , my_model : customllm):\n",
        "    #retrieving data\n",
        "    print(\"retrieving data from database.........\")\n",
        "    data_sql = my_db.retrieve_sql_data(question = question)\n",
        "    data_ddl = my_db.retrieve_ddl_data(question = question)\n",
        "    data_docs = my_db.retrieve_docs_data(question = question)\n",
        "    data_benchmark = my_db.retrieve_benchmark_data(question = question)\n",
        "    #checking if question already exists\n",
        "    print(\"checking if query already exists.........\")\n",
        "    sql_query = check_question_db(question = question , data_sql = data_sql)\n",
        "\n",
        "    # if no existing question exists\n",
        "    if sql_query is None:\n",
        "        print(\"query doesnt exist,Calling llm model.........\")\n",
        "        prompt = my_model.ask(question = question , sql_data = data_sql , ddl_data = data_ddl , docs_data = data_docs )\n",
        "        sql_query = my_model.submit_prompt(prompt , temperature = 0.1)\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        print(\"Query generated,executing query...........\")\n",
        "        sqlglot.transpile(sql_query)\n",
        "        df = execute_query(sql_query = sql_query)\n",
        "    except Exception as e:\n",
        "        print(\"Wrong query generated!\")\n",
        "        print(\"Trying again......\")\n",
        "        try:\n",
        "            sql_query = my_model.regenerate_query(prompt = prompt , error = e , query = sql_query , temperature = 0.3)\n",
        "            df = execute_query(sql_query = sql_query)\n",
        "        except:\n",
        "            print(sql_query)\n",
        "            print(\"Execution failed.Sending this report to administrator....\")\n",
        "            return\n",
        "\n",
        "\n",
        "    if df.shape[0] == 0:\n",
        "        print(sql_query)\n",
        "        print(\"Sorry the SQL query generated was correct but no data found!\")\n",
        "    else:\n",
        "        print(\"The query generated is : \\n\")\n",
        "        print(sql_query)\n",
        "        print(\"\\n\")\n",
        "        print(\"------------------------------------------------------------------------------------------------------------\\n\")\n",
        "        print(df)\n",
        "        print(\"\\n\")\n",
        "        try:\n",
        "            summary = my_model.generate_summary(question = question , df = df ,benchmark = data_benchmark , temperature = 0.4 , max_tokens = 500)\n",
        "            print(summary)\n",
        "        except:\n",
        "            pass\n",
        "        print(\"\\n\")\n",
        "        if df.shape[0] > 1:\n",
        "            try:\n",
        "                df.reset_index(drop=True, inplace=True)\n",
        "                plotly_code = my_model.generate_plotly_code(\n",
        "                                    question=question,\n",
        "                                    sql=sql_query,\n",
        "                                    df_metadata=f\"Running df.dtypes gives:\\n {df.dtypes}\", temperature = 0.3\n",
        "                                )\n",
        "                fig = my_model.get_plotly_figure(plotly_code=plotly_code, df=df)\n",
        "                plt.figure(figsize=(5,5))\n",
        "                if fig is not None:\n",
        "                    fig.show()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        follow_up_questions = []\n",
        "        try:\n",
        "            l = my_model.generate_followup_questions(question = question , sql = sql_query , df = df)\n",
        "            for i in l:\n",
        "                sql_data = my_db.retrieve_sql_data(question = i)\n",
        "                questions = [meta['question'] for metadata_list in sql_data['metadatas'] for meta in metadata_list]\n",
        "                for q in questions:\n",
        "                    if q not in follow_up_questions:\n",
        "                        follow_up_questions.append(q)\n",
        "        except:\n",
        "            for i in data_sql['metadatas']:\n",
        "                for k in i:\n",
        "                    if k['question'] != question and k['question'] not in follow_up_questions:\n",
        "                        follow_up_questions.append(k['question'])\n",
        "\n",
        "        for i in follow_up_questions:\n",
        "            print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BsYaAluAAZiu"
      },
      "outputs": [],
      "source": [
        "#initiallising the database\n",
        "question_texts , sql = modify_data(dataset)\n",
        "my_db = chroma_db(client_type = 'temporary')\n",
        "my_db.add_sql(question_texts = question_texts , sql = sql)\n",
        "my_db.add_ddl(ddl = ddl)\n",
        "my_db.add_docs(docs = docs)\n",
        "my_db.add_benchmark(benchmark = benchmark)\n",
        "\n",
        "#initialising model\n",
        "my_model = customllm(base_url = \"https://api.endpoints.anyscale.com/v1\" ,api_key = \"\" , model = \"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZlK30HtpA07S",
        "outputId": "c09b86ef-4c50-40fc-d80f-8809d8d26522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "retrieving data from database.........\n",
            "checking if query already exists.........\n",
            "query doesnt exist,Calling llm model.........\n",
            "Query generated,executing query...........\n",
            "The query generated is : \n",
            "\n",
            " select date_trunc('month', created_at) as month,\n",
            "            count(distinct id) as orders\n",
            "\n",
            "            from orders\n",
            "            where date_trunc('month', created_at) between current_date-3*interval '1 month' and current_date\n",
            "            group by 1 \n",
            "\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "       month  orders\n",
            "0 2024-01-01    5145\n",
            "1 2024-02-01    5477\n",
            "\n",
            "\n",
            " 3-month average orders: 5311\n",
            "\n",
            "Last 3-months orders: [5145, 5477, <avg: 5311>]\n",
            "\n",
            "<~Instruction> \n",
            "[You should always refer the DINST instruction for formatting. In this question user wants to know monthly total orders for last 3 months. So, I have calculated 3-month average orders and last 3-months orders. The user can refer this as a benchmark for interpreting the results. The straightforward answer is that last 3-months orders is [5145, 5477, 5311].]\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"dad63ff4-02f9-4f74-89c3-96298318c3d9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"dad63ff4-02f9-4f74-89c3-96298318c3d9\")) {                    Plotly.newPlot(                        \"dad63ff4-02f9-4f74-89c3-96298318c3d9\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"month=%{x}\\u003cbr\\u003eorders=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[5145.0,5477.0],\"textposition\":\"auto\",\"x\":[\"2024-01-01T00:00:00\",\"2024-02-01T00:00:00\"],\"xaxis\":\"x\",\"y\":[5145,5477],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"month\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"orders\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dad63ff4-02f9-4f74-89c3-96298318c3d9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "does more discount mean more orders?\n",
            "total number of orders for the last 30 days\n",
            "find the store wise number of orders for the last 30 days\n",
            "What is the average order value for each product type in the last 30 days?\n",
            "what is average time interval between two orders for last 90 days?\n",
            "what is average time interval between first and second order for last 180 days?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#running\n",
        "question = \"what is my monthly total orders for last 3 months?\"\n",
        "run(question = question , my_db = my_db , my_model = my_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
